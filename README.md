# Credit_Card_Default_Detection
The purpose of this project is to train a model on the dataset
generated by a financial institution. The dataset is about their
existing customers with the details including history of whether
they have defaulted or not. This model is then tested on the
new customers who have joined and are applying for credit
cards to determine whether they will default or not to decide
on issuance of credit cards.

The dataset consists of 13,444 observations with 14 variables. It includes CARDHLDR (It is a dummy variable,
which state 1 if application of credit card is accepted, 0
if not), AGE (age of consumers in years plus twelfths of
a year), ACADMOS (state the number of months living
at current address), ADEPCNT (the number of dependents
of consumers), MAJORDRG (number of major derogatory
reports), MINORDRG (Number of minor derogatory reports),
OWNRENT (which consumers owns or rent a house, 1 if
owns their home, 0 if rent), INCOME ( monthly income
of clients (divided by 10,000), SELFEMPL (state that the
consumer is self-employed or not - 1 if self-employed, 0 if not),
INCPER (state the Income divided by number of dependents),
EXPINC (it is a Ratio of monthly credit card expenditure
to yearly income), SPENDING (Average monthly credit card
expenditure only for CARDHOLDER = 1), LOGSPEND (it is
the logarithmic value of spending), DEFAULT (defaulted(1) or
not(0) â€“ the target variable(observed when CARDHLDR = 1,
10,499 observations) , if the there is a default on credit card
or not).

![image](https://github.com/Nikhil9425/Credit_Card_Default_Detection/assets/68101064/db602db4-6628-4082-aa9c-16022fb694b2)


The graph provided below gives a comparison of all the
six machine learning algorithms used in the project with
respect to their accuracy arranged according to their accuracy
in descending order.

![image](https://github.com/Nikhil9425/Credit_Card_Default_Detection/assets/68101064/72113409-c830-48bd-8ffb-1d207e9b4107)

After implementing all the six algorithms and then comparing each other based on their accuracy, we could see that XGBoost and ANN performed better followed by AdaBoost.
In general, we can say that models trained based on both the
boosting algorithms showed higher accuracy when compared
to other machine learning models such as Logistic Regression
and K-Nearest Neighbor

